---
layout: post
title:      "Wikipedia CLI Gem Portfolio Project"
date:       2019-06-28 19:38:55 +0000
permalink:  wikipedia_cli_gem_portfolio_project
---


My first project for the full stack web development program was to make a gem with a CLI to access the information of a scraped web page. I went about picking the web page by just thinking about what my favorite website was, and that turned out to be Wikipedia, and as I looked into it, it seemed like a perfect page to scrape. Even though the articles can be user written, Wikipedia's html still seemed to have a uniform structure. To begin it, I began thinking about how I wanted it structured. I used bundler to build out a basic gem directory, then built all the text files for the different classes and put comments in them describing their job. Then I decided I wanted to get the scaper class functioning at a basic level before dealing with the CLI. To begin the scraper class I looked for a basic wiki article to start with and found the "Flags of New York City" to be simple enough, and built my program around that. 

I set the bin file to call the scrape class method in the begining and would use the bin to call whatever method needed imediate testing in the future, so I would have only one thing to type into the terminal to test my program.  I then opened up the webpage with Nokogiri, saved it to a variable, and put in a pry to mess around with the page and see what I could find. I decided to try and scrap the main article itself before dealing with the infobox, so I looked at the page source and messed with the nokogiri output until it gave me just the information for the main article, which turned out to be an array with each paragraph and section name being a value in it. Each value had an HTML tag, the regular paragraphs had a p tag and the sections listed in the contents section had an H2 tag. Each page has an introductory paragraph, and its title is the only H1 tag on the page. So I decided to iterate through the page's array and use all the information before the first H2 tag, and put it into an introductory section with the H1 text as its title. Once it hit an H2 tag, it would create a new section, use that text as its title, and continue collecting the information. At first I built it to only detect p tags, but as I looked at other pages with different types of text styles like block quotes or lists, I would specifically add them in. I figure this way, if it encounters something it doesn't recognise, it will simply do nothing, leaving room for my program to be missing something, but not break.

After this I decided it was time to build the CLI which would turn out to be a prompt asking for the name of an article. At first I wanted to scrape the main page an display that as a sort of suggestion page, but I honestly never use the main page and don't know anyone who does, so I figured doing that would just cluter the program. So to begin, I changed the bin file to open the CLI method to begin testing. Then I built out my own contents page using the titles stored in the sections and let the user choose which section to go to after choosing the article they wanted to view. The CLI would only display one section at a time to avoid clutter. After I pretty much had a working program with everything displaying in a neat, readable fashion, I did a light search for bugs, then continued onto the infobox part. I figured the infobox section would be such a small feature I could wait on really hunting situations where the main section would go wrong.

However, the infobox section ended up being a lot harder then I thought it would be. Although having an array of all the pieces of text was the same, the sections of text were not named any different from each other. Pictures had the same tag as descriptions. So essentially I had to do a lot of pry and searching through the HTML to find the difference one level down. Eventually I found that the graphics shared an element with titles and sections names, which was the element "colspan". So I made it not detect colspan elements unless it had text one layer down, like section names had, but not pictures. It worked like a charm.

Finally to finish up the project, I refactored obsolete code, squished all the bugs I noticed, and wrote up the documentation and did a final check on everything to make sure it was working.


